{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import chardet\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv', sep=',', encoding='latin-1', on_bad_lines='skip', low_memory=False)\n",
    "analysis = [\"OR Result\", \"Confluence Inter Result\", \"OA Inter Result\", \"DF Inter Result\", \"PDG Result\"]\n",
    "result_analysis = [\"Confluence Inter\", \"OA Inter\", \"left right DFP-Inter\", \"right left DFP-Inter\", \"left right PDG\", \"right left PDG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Confluence Inter\"] != \"not-found\"]\n",
    "df = df[df[\"LOI\"] != \"-\"]\n",
    "print(df[\"Confluence Inter\"].count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_analysis(list_name):\n",
    "    names = []\n",
    "    for i in list_name:\n",
    "        if (\"Confluence\" in i):\n",
    "            names.append(\"Confluence Inter\")\n",
    "        elif (\"OA\" in i):\n",
    "            names.append(\"OA Inter\")\n",
    "        elif (\"DF\" in i):\n",
    "            names.append(\"left right DFP-Inter\")\n",
    "            names.append(\"right left DFP-Inter\")\n",
    "        elif (\"PDG\" in i):\n",
    "            names.append(\"left right PDG\")\n",
    "            names.append(\"right left PDG\")\n",
    "\n",
    "    return names\n",
    "\n",
    "def get_reverse_name(lists):\n",
    "    names = []\n",
    "    for elem_list in lists:\n",
    "        aux_list = []\n",
    "        for i in elem_list:\n",
    "            if (\"Confluence Inter\" in i):\n",
    "                aux_list.append(\"CF\")\n",
    "            elif (\"OA Inter\" in i):\n",
    "                aux_list.append(\"OA\")\n",
    "            elif (\"left right DFP-Inter\" in i):\n",
    "                aux_list.append(\"DF\")\n",
    "            elif (\"left right PDG\" in i):\n",
    "                aux_list.append(\"PDG\")\n",
    "        names.append(aux_list)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dos elementos\n",
    "elements = [\"Confluence\", \"OA\", \"DF\", \"PDG\"]\n",
    "combinations_list = []\n",
    "# Gerar todas as combinações possíveis de 2 a 4 elementos sem repetições\n",
    "for length in range(1, len(elements) + 1):\n",
    "    for combination in combinations(elements, length):\n",
    "        combinations_list.append(list(combination))\n",
    "\n",
    "print(combinations_list)\n",
    "\n",
    "# gerando todas as combinações possíveis\n",
    "\n",
    "analysis_combination = []\n",
    "for i in combinations_list:\n",
    "    analysis_combination.append(get_name_analysis(i))\n",
    "print(analysis_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix(columns):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        values = [row[column] for column in columns]\n",
    "        loi = row[\"LOI\"]\n",
    "        or_value = any(value != 'FALSE' for value in values)\n",
    "        result = \"\"\n",
    "        if or_value == True and loi == 'Yes':\n",
    "            result = \"TRUE POSITIVE\"\n",
    "        elif or_value == False and loi == 'No':\n",
    "            result = \"TRUE NEGATIVE\"\n",
    "        elif or_value == False and loi == 'Yes':\n",
    "            result = \"FALSE NEGATIVE\"\n",
    "        elif or_value == True and loi == 'No':\n",
    "            result = \"FALSE POSITIVE\"\n",
    "        \n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando se o OR da planilha é igual ao da função\n",
    "\n",
    "r = calculate_matrix(result_analysis)\n",
    "# print(r)\n",
    "\n",
    "lista = [i for i in df[\"OR Result\"]]\n",
    "\n",
    "if r == lista:\n",
    "    print(\"Iguais\")\n",
    "\n",
    "# print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algoritmo\n",
    "#Se todos os TP e TN de uma análise A1 são também TP e TN de outra análise A2, e A2 tem pelo \n",
    "#menos um TP ou TN a mais que A1, o algoritmo indica que a gente deve ficar apenas com A2\n",
    "\n",
    "#Se todos os TP e TN de uma análise A1 são também TP e TN de outra análise A2, e A2 não tem \n",
    "#nenhum TP ou TN a mais que A1, o algoritmo indica que a gente deve ficar com a mais rápida entre A1 e A2\n",
    "\n",
    "#Se nem todos os TP e TN de uma análise A1 são também TP e TN de outra análise A2, e vice-versa, \n",
    "#o algoritmo sugere a gente ficar com o OR de A1 e A2\n",
    "\n",
    "#Se todos os TP e TN de uma análise (CONJUNTO - OU de n análise) são também TP e TN de outra análise A2, e A2 tem pelo \n",
    "#menos um TP ou TN a mais que A1, o algoritmo indica que a gente deve ficar apenas com A2\n",
    "\n",
    "def compare_algorithm(list1, list2):\n",
    "    TPIgual = True\n",
    "    TNIgual = True\n",
    "    for (l1, l2) in zip(list1, list2):\n",
    "        if (l1 == \"TRUE POSITIVE\" and l1 != l2):\n",
    "            TPIgual = False\n",
    "        if (l1 == \"TRUE NEGATIVE\" and l1 != l2):\n",
    "            TNIgual = False\n",
    "\n",
    "\n",
    "    if (TPIgual and TNIgual and (list2.count(\"TRUE POSITIVE\") > list1.count(\"TRUE POSITIVE\") or list2.count(\"TRUE NEGATIVE\") > list1.count(\"TRUE NEGATIVE\"))):\n",
    "        print(\"Vamos optar por Last!\")\n",
    "    elif (TPIgual and TNIgual and (list2.count(\"TRUE POSITIVE\") == list1.count(\"TRUE POSITIVE\") or list2.count(\"TRUE NEGATIVE\") == list1.count(\"TRUE NEGATIVE\"))):\n",
    "        print(\"Ficar com a mais rápida!\")\n",
    "    else:\n",
    "        print(\"Vamos optar pelo First!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fp_fn(list_result):\n",
    "    # Criar um contador dos elementos da lista\n",
    "    element_count = Counter(list_result)\n",
    "\n",
    "    result = []\n",
    "    # Imprimir a contagem de elementos repetidos\n",
    "    for element, count in element_count.items():\n",
    "        if count > 1:\n",
    "            result.append((str(element)+\": \"+str(count)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Longest:\n",
    "    maiorPrecision = -1.0\n",
    "    maiorRecall = -1.0\n",
    "    maiorF1 = -1.0\n",
    "    maiorAcuracia = -1.0\n",
    "    mPrecision = []\n",
    "    mRecall = []\n",
    "    mF1 = []\n",
    "    mAcuracia = []\n",
    "    mPrecision_t = []\n",
    "    mRecall_t = []\n",
    "    mF1_t = []\n",
    "    mAcuracia_t = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maiorPrecision = -1.0\n",
    "        self.maiorRecall = -1.0\n",
    "        self.maiorF1 = -1.0\n",
    "        self.maiorAcuracia = -1.0\n",
    "        mPrecision = []\n",
    "        mRecall = []\n",
    "        mF1 = []\n",
    "        mAcuracia = []\n",
    "        mPrecision_t = []\n",
    "        mRecall_t = []\n",
    "        mF1_t = []\n",
    "        mAcuracia_t = []\n",
    "\n",
    "\n",
    "    def confusion_matrix(self, options, values_elem):\n",
    "\n",
    "        # Inicializar as variáveis\n",
    "        tp = None\n",
    "        fp = None\n",
    "        tn = None\n",
    "        fn = None\n",
    "        \n",
    "        # Extrair os valores dos elementos da lista\n",
    "        for option in options:\n",
    "            if \"TRUE POSITIVE\" in option:\n",
    "                tp = int(option.split(': ')[1])\n",
    "            elif \"FALSE POSITIVE\" in option:\n",
    "                fp = int(option.split(': ')[1])\n",
    "            elif \"TRUE NEGATIVE\" in option:\n",
    "                tn = int(option.split(': ')[1])\n",
    "            elif \"FALSE NEGATIVE\" in option:\n",
    "                fn = int(option.split(': ')[1])\n",
    "\n",
    "        # Calcular as métricas se todos os valores foram extraídos\n",
    "        if tp is not None and fp is not None and tn is not None and fn is not None:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "            if (precision > self.maiorPrecision):\n",
    "                self.maiorPrecision = precision\n",
    "                self.mPrecision = []\n",
    "                self.mPrecision.append(values_elem)\n",
    "\n",
    "            if (precision == self.maiorPrecision):\n",
    "                if values_elem not in self.mPrecision:\n",
    "                    self.mPrecision.append(values_elem)\n",
    "                \n",
    "            if (recall > self.maiorRecall):\n",
    "                self.maiorRecall = recall\n",
    "                self.mRecall = []\n",
    "                self.mRecall.append(values_elem)\n",
    "\n",
    "            if (recall == self.maiorRecall):\n",
    "                if values_elem not in self.mRecall:\n",
    "                    self.mRecall.append(values_elem)\n",
    "\n",
    "                \n",
    "            if (f1_score > self.maiorF1):\n",
    "                self.maiorF1 = f1_score\n",
    "                self.mF1 = []\n",
    "                self.mF1.append(values_elem)\n",
    "\n",
    "            if (f1_score == self.maiorF1):\n",
    "                if values_elem not in self.mF1:\n",
    "                    self.mF1.append(values_elem)\n",
    "            \n",
    "            if (accuracy > self.maiorAcuracia):            \n",
    "                self.maiorAcuracia = accuracy\n",
    "                self.mAcuracia = []\n",
    "                self.mAcuracia.append(values_elem)\n",
    "\n",
    "            if (accuracy == self.maiorAcuracia):\n",
    "                if values_elem not in self.mAcuracia:\n",
    "                    self.mAcuracia.append(values_elem)\n",
    "\n",
    "            # Imprimir as métricas\n",
    "            print(f\"Precision: {precision:.2f}\")\n",
    "            print(f\"Recall: {recall:.2f}\")\n",
    "            print(f\"F1 Score: {f1_score:.2f}\")\n",
    "            print(f\"Accuracy: {accuracy:.2f}\")\n",
    "                \n",
    "            result_metrics = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1_score,\n",
    "                \"accuracy\": accuracy\n",
    "            }\n",
    "            return result_metrics\n",
    "        else:\n",
    "            print(\"Não foi possível extrair todos os valores necessários.\")\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolhendo qual o melhor resultado com base no Algoritmo de comparação\n",
    "best = Longest()\n",
    "\n",
    "for first in analysis_combination:\n",
    "    for last in analysis_combination:\n",
    "        if first != last:  \n",
    "            print(first, last)      \n",
    "            r_first = calculate_matrix(first)\n",
    "            r_last = calculate_matrix(last)\n",
    "\n",
    "            print(\"Comparing OR of the first\", first, \"with the last\", last)\n",
    "            print(\"First:\", count_fp_fn(r_first))\n",
    "            best.confusion_matrix(count_fp_fn(r_first), first)\n",
    "            print(\"Last: \", count_fp_fn(r_last))\n",
    "            best.confusion_matrix(count_fp_fn(r_last), last)\n",
    "            compare_algorithm(r_first, r_last)\n",
    "            print()\n",
    "\n",
    "print(f\"Precision: {best.maiorPrecision:.2f}\", best.mPrecision)\n",
    "print(f\"Recall: {best.maiorRecall:.2f}\", best.mRecall)\n",
    "print(f\"F1-score: {best.maiorF1:.2f}\", best.mF1)\n",
    "print(f\"Accuracy: {best.maiorAcuracia:.2f}\", best.mAcuracia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Metric': ['Precision', 'Recall', 'F1-score', 'Accuracy'],\n",
    "    'Value': [round(best.maiorPrecision, 2), round(best.maiorRecall, 2), round(best.maiorF1, 2), round(best.maiorAcuracia, 2)],\n",
    "   'Analyses': [get_reverse_name(best.mPrecision), get_reverse_name(best.mRecall), get_reverse_name(best.mF1), get_reverse_name(best.mAcuracia)]\n",
    "}\n",
    "dframe = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=dframe.values, colLabels=dframe.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "for i, col in enumerate(dframe.columns):\n",
    "    col_width = max([len(str(val)) for val in dframe[col]])\n",
    "    table.auto_set_column_width(i)\n",
    "    table.auto_set_column_width(col_width)\n",
    "\n",
    "plt.title(\"Result of the best combinations\", y=0.8)\n",
    "\n",
    "plt.savefig('best_combinations.jpg', dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Confluence Inter\"] != \"not-found\"]\n",
    "df = df[df[\"LOI\"] != \"-\"]\n",
    "print(df[\"Confluence Inter\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_exclusive(op1, op2, val):\n",
    "    \n",
    "    count = 0\n",
    "    for i, j in zip(op1, op2):\n",
    "        if (val in i and (val not in j)):\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_algorithm_tp_fp(first, last):\n",
    "    first_tp = check_exclusive(first, last, \"TRUE POSITIVE\")\n",
    "    first_fp = check_exclusive(first, last, \"FALSE POSITIVE\")\n",
    "    r_first = first_tp-first_fp\n",
    "    print(\"TP exclusive first:\", first_tp, \", FP exclusive first:\", first_fp, \", saldo:\", r_first)\n",
    "    \n",
    "    last_tp = check_exclusive(last, first, \"TRUE POSITIVE\")\n",
    "    last_fp = check_exclusive(last, first, \"FALSE POSITIVE\")\n",
    "    r_last = last_tp-last_fp\n",
    "    print(\"TP exclusive last:\", last_tp, \", FP exclusive last:\", last_fp, \", saldo:\", r_last)\n",
    "\n",
    "    if (r_first > r_last):\n",
    "        print(\"Escolher first (OR)!\")\n",
    "    elif (r_first < r_last):\n",
    "        print(\"Escolher last!\")\n",
    "    else:\n",
    "        print(\"Ficar com a mais rápida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolhendo qual o melhor resultado com base na tabela de TP - FP\n",
    "obj = Longest()\n",
    "for first_elements in analysis_combination: \n",
    "    for last_element in analysis_combination:\n",
    "        if first_elements != last_element:\n",
    "            # print(i, first_elements, last_element)\n",
    "            r_first = calculate_matrix(first_elements)\n",
    "            r_last = calculate_matrix(last_element)\n",
    "\n",
    "            print(\"Comparing OR of the first\", first_elements, \"with the last\", last_element)\n",
    "            print(\"First:\", count_fp_fn(r_first))\n",
    "            \n",
    "            obj.confusion_matrix(count_fp_fn(r_first), first_elements)\n",
    "            print(\"Last: \", count_fp_fn(r_last))\n",
    "            obj.confusion_matrix(count_fp_fn(r_last), last_element)\n",
    "\n",
    "            check_algorithm_tp_fp(r_first, r_last)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular pontuação para cada resultado\n",
    "def calculate_score(metrics, weights):\n",
    "    return sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "\n",
    "def check_algorithm_weights(first, last, precision, recall, f1_score, accuracy):\n",
    "    obj = Longest()\n",
    "    result_1_metrics = obj.confusion_matrix(count_fp_fn(first), first)\n",
    "    result_2_metrics = obj.confusion_matrix(count_fp_fn(last), last)\n",
    "    \n",
    "    # Definir pesos para as métricas (você pode ajustar esses pesos)\n",
    "    weights = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    score_1 = calculate_score(result_1_metrics, weights)\n",
    "    score_2 = calculate_score(result_2_metrics, weights)\n",
    "\n",
    "    print(f\"Score First: {score_1:.2f}\")\n",
    "    print(f\"Score Last: {score_2:.2f}\")\n",
    "\n",
    "    # Determinar qual resultado é melhor com base na pontuação\n",
    "    if score_1 > score_2:\n",
    "        best_result = \"First\"\n",
    "    else:\n",
    "        best_result = \"Last\"\n",
    "\n",
    "    print(f\"O melhor resultado é {best_result}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolhendo qual o melhor resultado com base na tabela de TP - FP\n",
    "\n",
    "for first_elements in analysis_combination:\n",
    "    for last_element in analysis_combination:\n",
    "        if first_elements != last_element:\n",
    "                    \n",
    "            r_first = calculate_matrix(first_elements)\n",
    "            r_last = calculate_matrix(last_element)\n",
    "\n",
    "            print(\"Comparing OR of the first\", first_elements, \"with the last\", last_element)\n",
    "            print(\"First:\", count_fp_fn(r_first))\n",
    "            print(\"Last: \", count_fp_fn(r_last))\n",
    "\n",
    "            # \"precision\": 0.4\n",
    "            # \"recall\": 0.3\n",
    "            # \"f1_score\": 0.2\n",
    "            # \"accuracy\": 0.1\n",
    "            check_algorithm_weights(r_first, r_last, 0.4, 0.3, 0.2, 0.1)\n",
    "\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo de TP e FP exclusivo para cada análise\n",
    "\n",
    "def check_exclusivo(op1, op2, op3, op4, val):\n",
    "    if (val in op1 and (val not in op2 and val not in op3 and val not in op4)):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "analysis = [\"OR Result\", \"Confluence Inter Result\", \"OA Inter Result\", \"DF Inter Result\", \"PDG Result\"]\n",
    "\n",
    "cf_tp, df_tp, oa_tp, pdg_tp = 0, 0, 0, 0\n",
    "\n",
    "cf_fp, df_fp, oa_fp, pdg_fp = 0, 0, 0, 0\n",
    "\n",
    "for (cf, df, oa, pdg) in zip(df[\"Confluence Inter Result\"], df[\"DF Inter Result\"], df[\"OA Inter Result\"], df[\"PDG Result\"]):\n",
    "    if (\"nan\" not in str(cf)):\n",
    "\n",
    "        cf_tp = cf_tp + check_exclusivo(cf, df, oa, pdg, \"TRUE POSITIVE\")\n",
    "        df_tp = df_tp + check_exclusivo(df, cf, oa, pdg, \"TRUE POSITIVE\")\n",
    "        oa_tp = oa_tp + check_exclusivo(oa, df, cf, pdg, \"TRUE POSITIVE\")\n",
    "        pdg_tp = pdg_tp + check_exclusivo(pdg, cf, df, oa, \"TRUE POSITIVE\")\n",
    "\n",
    "        cf_fp = cf_fp + check_exclusivo(cf, df, oa, pdg, \"FALSE POSITIVE\")\n",
    "        df_fp = df_fp + check_exclusivo(df, cf, oa, pdg, \"FALSE POSITIVE\")\n",
    "        oa_fp = oa_fp + check_exclusivo(oa, df, cf, pdg, \"FALSE POSITIVE\")\n",
    "        pdg_fp = pdg_fp + check_exclusivo(pdg, cf, df, oa, \"FALSE POSITIVE\")\n",
    "\n",
    "data = {\n",
    "    'Analysis': ['CF', 'OA', 'DF', 'PDG'],\n",
    "    'Exclusive TP': [cf_tp, oa_tp, df_tp,  pdg_tp],\n",
    "    'Exclusive FP': [cf_fp, oa_fp, df_fp,  pdg_fp]\n",
    "}\n",
    "\n",
    "dframe = pd.DataFrame(data)\n",
    "dframe['Balance (TPe-FPe)'] = dframe['Exclusive TP'] - dframe['Exclusive FP']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=dframe.values, colLabels=dframe.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "plt.title(\"Exclusive analyses\", y=0.8)\n",
    "plt.savefig('exclusive_analysis.jpg', dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "df_t = pd.read_csv('resultTime.csv', sep=';', encoding='utf-8', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "# analysis = [\"OR Result\", \"Confluence Inter Result\", \"OA Inter Result\", \"DF Inter Result\", \"PDG Result\"]\n",
    "# result_analysis = [\"Confluence Inter\", \"OA Inter\", \"left right DFP-Inter\", \"right left DFP-Inter\", \"left right PDG\", \"right left PDG\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_analysis_time(list_name, config):\n",
    "    names = []\n",
    "    for i in list_name:\n",
    "        if (\"Confluence\" in str(i) and \"Config CF left-base\" not in names and \"Confluence left-base\" not in names):\n",
    "            if config:\n",
    "                names.append(\"Config CF left-base\")\n",
    "                names.append(\"Config CF right-base\")\n",
    "            names.append(\"Confluence left-base\")\n",
    "            names.append(\"Confluence right-base\")\n",
    "        elif (\"OA\" in str(i)):\n",
    "            if config:\n",
    "                names.append(\"Config OA\")\n",
    "            names.append(\"OA\")\n",
    "        elif (\"DF\" in str(i) and \"Config DFP left\" not in names and \"DFP left\" not in names):\n",
    "            if config:\n",
    "                names.append(\"Config DFP left\")\n",
    "                names.append(\"Config DFP right\")\n",
    "            names.append(\"DFP left\")\n",
    "            names.append(\"DFP right\")\n",
    "        elif (\"PDG\" in str(i) and \"Config PDG left\" not in names and \"PDG left\" not in names):\n",
    "            if config:\n",
    "                names.append(\"Config PDG left\")\n",
    "                names.append(\"Config PDG right\")\n",
    "            names.append(\"PDG left\")\n",
    "            names.append(\"PDG right\")\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_n_esimo_element(lista_de_listas, n):\n",
    "    return sum(sublista[n] for sublista in lista_de_listas)\n",
    "\n",
    "def get_sum_all_list(all_list):\n",
    "    return [sum_n_esimo_element(all_list, i) for i in range(len(all_list[0]))]\n",
    "\n",
    "def get_total_time(list_analysis):\n",
    "    result = []\n",
    "    for column_analysis in list_analysis:\n",
    "        actual_time = [i for i in df_t[column_analysis]]\n",
    "        result.append(actual_time)\n",
    "    return result\n",
    "\n",
    "def get_mean(values):\n",
    "    return pd.Series(get_sum_all_list(values)).mean()\n",
    "\n",
    "def get_median(values):\n",
    "    return pd.Series(get_sum_all_list(values)).median()\n",
    "\n",
    "def get_sum(values):\n",
    "    return pd.Series(get_sum_all_list(values)).sum()\n",
    "\n",
    "def get_standard_desviation(values):\n",
    "    return pd.Series(get_sum_all_list(values)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_metric(metric, config):\n",
    "    metric_t = []\n",
    "    for i in metric:\n",
    "        aux_list = get_name_analysis_time(i, config)\n",
    "        result = get_total_time(aux_list)\n",
    "        value = round(pd.Series(get_mean(result)).sum(), 2)\n",
    "        metric_t.append(value)\n",
    "    return metric_t\n",
    "\n",
    "def get_sum_metric(metric, config):\n",
    "    metric_t = []\n",
    "    for i in metric:\n",
    "        aux_list = get_name_analysis_time(i, config)\n",
    "        result = get_total_time(aux_list)\n",
    "        value = round(pd.Series(get_sum(result)).sum(), 2)\n",
    "        metric_t.append(value)\n",
    "    return metric_t\n",
    "\n",
    "def get_median_metric(metric, config):\n",
    "    metric_t = []\n",
    "    for i in metric:\n",
    "        aux_list = get_name_analysis_time(i, config)\n",
    "        result = get_total_time(aux_list)\n",
    "        value = round(pd.Series(get_median(result)).sum(), 2)\n",
    "        metric_t.append(value)\n",
    "    return metric_t\n",
    "\n",
    "def get_std_metric(metric, config):\n",
    "    metric_t = []\n",
    "    for i in metric:\n",
    "        aux_list = get_name_analysis_time(i, config)\n",
    "        result = get_total_time(aux_list)\n",
    "        value = round(pd.Series(get_standard_desviation(result)).sum(), 2)\n",
    "        metric_t.append(value)\n",
    "    return metric_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with_config = True\n",
    "\n",
    "colums = df_t.columns\n",
    "\n",
    "print(\"Analyzing\", best.mPrecision)\n",
    "mean_p = get_mean_metric(best.mPrecision, with_config)\n",
    "median_p = get_median_metric(best.mPrecision, with_config)\n",
    "# sum_p = get_sum_metric(best.mPrecision, with_config)\n",
    "std_p = get_std_metric(best.mPrecision, with_config)\n",
    "\n",
    "# out_precision = f\"Mean: {mean_p} Median: {median_p} Sum: {sum_p} Standard: {std_p}\"\n",
    "out_precision = [mean_p, median_p, std_p]\n",
    "\n",
    "print(out_precision)\n",
    "\n",
    "print(\"Analyzing\", best.mRecall)\n",
    "mean_p = get_mean_metric(best.mRecall, with_config)\n",
    "median_p = get_median_metric(best.mRecall, with_config)\n",
    "# sum_p = get_sum_metric(best.mRecall, with_config)\n",
    "std_p = get_std_metric(best.mRecall, with_config)\n",
    "\n",
    "# out_recall = f\"Mean: {mean_p} Median: {median_p} Sum: {sum_p} Standard: {std_p}\"\n",
    "out_recall = [mean_p, median_p, std_p]\n",
    "\n",
    "print(out_recall)\n",
    "\n",
    "print(\"Analyzing\", best.mF1)\n",
    "\n",
    "mean_p = get_mean_metric(best.mF1, with_config)\n",
    "median_p = get_median_metric(best.mF1, with_config)\n",
    "# sum_p = get_sum_metric(best.mF1, with_config)\n",
    "std_p = get_std_metric(best.mF1, with_config)\n",
    "\n",
    "# out_f1 = f\"Mean: {mean_p} Median: {median_p} Sum: {sum_p} Standard: {std_p}\"\n",
    "out_f1 = [mean_p, median_p, std_p]\n",
    "print(out_f1)\n",
    "\n",
    "print(\"Analyzing\", best.mAcuracia)\n",
    "mean_p = get_mean_metric(best.mAcuracia, with_config)\n",
    "median_p = get_median_metric(best.mAcuracia, with_config)\n",
    "# sum_p = get_sum_metric(best.mAcuracia, with_config)\n",
    "std_p = get_std_metric(best.mAcuracia, with_config)\n",
    "\n",
    "# out_accuracy = f\"Mean: {mean_p}\\nMedian: {median_p}\\nSum: {sum_p}\\nStandard: {std_p}\\n\"\n",
    "out_accuracy = [mean_p, median_p, std_p]\n",
    "print(out_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_tuple(out):\n",
    "    result = []\n",
    "    for index in range(len(out[0])):\n",
    "        val = [x[index] for x in out]\n",
    "        result.append(tuple(val))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Metric': ['Precision', 'Recall', 'F1-score', 'Accuracy'],\n",
    "    'Value': [round(obj.maiorPrecision, 2), round(obj.maiorRecall, 2), round(obj.maiorF1, 2), round(obj.maiorAcuracia, 2)],\n",
    "    'Analyses': [get_reverse_name(obj.mPrecision), get_reverse_name(obj.mRecall), get_reverse_name(obj.mF1), get_reverse_name(obj.mAcuracia)],\n",
    "    'Time (s) (mean, median, standard)': [convert_list_to_tuple(out_precision), convert_list_to_tuple(out_recall), convert_list_to_tuple(out_f1), convert_list_to_tuple(out_accuracy)]\n",
    "}\n",
    "dframe = pd.DataFrame(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=dframe.values, colLabels=dframe.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "for i, col in enumerate(dframe.columns):\n",
    "    col_width = max([len(str(val)) for val in dframe[col]])\n",
    "    table.auto_set_column_width(i)\n",
    "    table.auto_set_column_width(col_width)\n",
    "\n",
    "plt.title(\"Result of the best combinations\", y=0.8)\n",
    "\n",
    "plt.savefig('best_combinations_time.jpg', dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(analysis_combination))\n",
    "print(analysis_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soot_results = pd.read_csv('soot-results.csv', sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)\n",
    "loi = pd.read_csv('LOI.csv', sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from fpdf import FPDF\n",
    "\n",
    "class ReportAnalysis:\n",
    "\n",
    "    def __init__(self, path_result, path_ground_truth):\n",
    "        self.soot_results = pd.read_csv(path_result, sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)\n",
    "        self.loi = pd.read_csv(path_ground_truth, sep=';', encoding='latin-1', on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "        self.generate_results()\n",
    "\n",
    "    def get_loi(self, project, class_name,  method, merge_commit):\n",
    "\n",
    "        filter_scenario = (self.loi['Project'] == str(project)) & (self.loi['Merge Commit'] == str(merge_commit)) & (self.loi['Class Name'] == str(class_name)) & (self.loi['Method or field declaration changed by the two merged branches'] == str(method))\n",
    "        value_LOI = \"\"\n",
    "\n",
    "        if filter_scenario.any():\n",
    "            value_LOI = self.loi.loc[filter_scenario, 'Locally Observable Interference'].values[0]\n",
    "\n",
    "        return value_LOI\n",
    "\n",
    "    def calculate_matrix_loi(self, columns):\n",
    "        results = []\n",
    "        info_LOI = ['project', 'class', 'method', 'merge commit']\n",
    "\n",
    "        for index, row in self.soot_results.iterrows():\n",
    "            list_values = self.soot_results.columns.tolist()\n",
    "            remove_columns = ['project', 'class', 'method', 'merge commit', 'Time']\n",
    "            list_values = [coluna for coluna in list_values if coluna not in remove_columns]\n",
    "            values = [row[column] for column in list_values]\n",
    "\n",
    "            values_LOI = [row[column] for column in info_LOI]\n",
    "\n",
    "            loi_actual = self.get_loi(*values_LOI)\n",
    "\n",
    "            or_value = any(str(value).lower() != 'false' for value in values)\n",
    "\n",
    "            result = \"\"\n",
    "            if or_value == True and loi_actual == 'Yes':\n",
    "                result = \"TRUE POSITIVE\"\n",
    "            elif or_value == False and loi_actual == 'No':\n",
    "                result = \"TRUE NEGATIVE\"\n",
    "            elif or_value == False and loi_actual == 'Yes':\n",
    "                result = \"FALSE NEGATIVE\"\n",
    "            elif or_value == True and loi_actual == 'No':\n",
    "                result = \"FALSE POSITIVE\"\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def generate_results(self):\n",
    "\n",
    "        print(\"Generating results...\")\n",
    "\n",
    "        FP,TP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "        list_columns = self.soot_results.columns.tolist()\n",
    "\n",
    "        result_matrix = self.calculate_matrix_loi(list_columns)\n",
    "\n",
    "        for elem, count in Counter(result_matrix).items():\n",
    "            if (elem == 'FALSE POSITIVE'):\n",
    "                FP = count\n",
    "            if (elem == 'FALSE NEGATIVE'):\n",
    "                FN = count\n",
    "            if (elem == 'TRUE POSITIVE'):\n",
    "                TP = count\n",
    "            if (elem == 'TRUE NEGATIVE'):\n",
    "                TN = count\n",
    "\n",
    "        sensitivity = 0 if ((TP + FN) == 0) else (TP / (TP + FN))\n",
    "        precision = 0 if ((TP + FP) == 0) else (TP / (TP + FP))\n",
    "        f1_score = 0 if ((2*TP + FP + FN) == 0) else (2*TP / (2*TP + FP + FN))\n",
    "        accuracy = 0 if ((FP + TP + TN + FN) == 0) else ((TP + TN) / (FP + TP + TN + FN))\n",
    "\n",
    "        # variable pdf\n",
    "        pdf = FPDF()\n",
    "\n",
    "        # add a page\n",
    "        pdf.add_page()\n",
    "\n",
    "        # set style and size of font\n",
    "        # that you want in the pdf\n",
    "        pdf.set_font(\"Arial\", size = 15)\n",
    "\n",
    "        # create a cell\n",
    "        pdf.cell(200, 10, txt = \"Results for execution\",\n",
    "                 ln = 1, align = 'C')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"Precision: \"+str(round(precision, 4))),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"Recall: \"+str(round(sensitivity, 4))),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"F1 Score: \"+str(round(f1_score, 4))),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"Accuracy: \"+str(round(accuracy, 4))),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"False Positives: \"+str(FP)),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"False Negatives: \"+str(FN)),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"True Positives: \"+str(TP)),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        pdf.cell(200, 10, txt = (\"True Negatives: \"+str(TN)),\n",
    "                 ln = 2, align = 'L')\n",
    "\n",
    "        cm = np.array([[TP,  FP], [FN, TN]])\n",
    "        normalize = False\n",
    "        target_names = ['Actually Positive', ' Actually Negative']\n",
    "        target_names2 = ['Predicted Positive', ' Predicted Negative']\n",
    "        title = \"Confusion Matrix\"\n",
    "\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.colorbar()\n",
    "\n",
    "        if target_names is not None:\n",
    "            tick_marks = np.arange(len(target_names))\n",
    "            plt.xticks(tick_marks, target_names, fontsize=16)\n",
    "            plt.yticks(tick_marks, target_names2, fontsize=16)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            if normalize:\n",
    "                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"yellow\" if cm[i, j] > thresh else \"black\", fontsize=23)\n",
    "            else:\n",
    "                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"yellow\" if cm[i, j] > thresh else \"black\", fontsize=23)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(\"confusion_matrix.jpg\")\n",
    "\n",
    "        pdf.image(\"confusion_matrix.jpg\", x = None, y = None, w = 160, h = 110, type = 'jpg', link = 'confusion_matrix.jpg')\n",
    "\n",
    "        # Save the pdf with name .pdf\n",
    "        pdf.output(\"output/data/results.pdf\")\n",
    "        # pdf.output(\"results.pdf\")\n",
    "\n",
    "        print(\"Results in output/data/results.pdf\")\n",
    "\n",
    "path_ground_truth = \"../miningframework/input/LOI.csv\"\n",
    "path_result = '../miningframework/output/data/soot-results.csv'\n",
    "# path_ground_truth = \"LOI.csv\"\n",
    "# path_result = 'soot-results.csv'\n",
    "\n",
    "print(\"Reading analyses execution results...\")\n",
    "\n",
    "ReportAnalysis(path_result, path_ground_truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def menor_tamanho_chave(dicionario):\n",
    "    menor_chave = None\n",
    "    menor_tamanho = float('inf')  # Inicialmente, consideramos o tamanho infinito\n",
    "\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) < menor_tamanho:\n",
    "            menor_chave = chave\n",
    "            menor_tamanho = len(valor)\n",
    "\n",
    "    return menor_tamanho\n",
    "def chaves_com_tamanho_n(dicionario, tamanho):\n",
    "    chaves = []\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) > tamanho:\n",
    "            chaves.append(chave)\n",
    "    return chaves\n",
    "\n",
    "def adjusting_dict(dados_dict):\n",
    "    # Chave que você deseja dividir\n",
    "    dicionario = dados_dict\n",
    "    \n",
    "    len_dic = menor_tamanho_chave(dados_dict)\n",
    "\n",
    "    chaves = chaves_com_tamanho_n(dados_dict, len_dic)\n",
    "    \n",
    "    for chave in chaves:\n",
    "        # Verifica se a chave existe no dicionário\n",
    "        if chave in dicionario:\n",
    "            # Obtém a lista de valores correspondente à chave\n",
    "            lista_valores = dicionario[chave]\n",
    "            \n",
    "            # Verifica se a lista tem mais de len_dic elementos\n",
    "            if len(lista_valores) > len_dic:\n",
    "                \n",
    "                # Divide a lista em duas partes de len_dic elementos cada\n",
    "                primeira_parte = lista_valores[:len_dic]\n",
    "                segunda_parte = lista_valores[len_dic:]\n",
    "                \n",
    "                # Cria duas novas chaves no dicionário com as partes divididas\n",
    "                dicionario[chave + ' left-right'] = primeira_parte\n",
    "                dicionario[chave + ' right-left'] = segunda_parte\n",
    "                \n",
    "                # Exclui a chave antiga\n",
    "                del dicionario[chave]\n",
    "            else:\n",
    "                dicionario[chave] = lista_valores\n",
    "                \n",
    "\n",
    "    # Exibe o dicionário atualizado\n",
    "    return dicionario\n",
    "\n",
    "\n",
    "def normalize_dict(dic):\n",
    "    # Encontre o tamanho máximo entre todas as listas\n",
    "    tamanho_maximo = max(len(valor) for valor in dic.values())\n",
    "    \n",
    "    # Percorra as chaves do dicionário\n",
    "    for chave, valor in dic.items():\n",
    "        if isinstance(valor, list):\n",
    "            # Se a lista for menor que o tamanho máximo, preencha com zeros\n",
    "            while len(valor) < tamanho_maximo:\n",
    "                valor.append(0)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "\n",
    "dados_dict = {}\n",
    "\n",
    "arquivo_entrada = 'time.txt'\n",
    "\n",
    "# Abrir o arquivo de entrada e ler as linhas\n",
    "with open(arquivo_entrada, 'r') as arquivo:\n",
    "    linhas = arquivo.readlines()\n",
    "\n",
    "# Processar as linhas e preencher o dicionário de dados\n",
    "for linha in linhas:\n",
    "    partes = linha.strip().split(';')\n",
    "    if len(partes) == 2:\n",
    "        coluna, valor = partes[0].strip(), partes[1].strip().replace(\"s\",\"\")        \n",
    "        if coluna in dados_dict:\n",
    "            dados_dict[coluna].append(valor)\n",
    "        else:\n",
    "            dados_dict[coluna] = [valor]\n",
    "\n",
    "aux_dic = adjusting_dict(dados_dict)\n",
    "\n",
    "final_dic = normalize_dict(aux_dic)\n",
    "\n",
    "# Criar um DataFrame com Pandas\n",
    "df = pd.DataFrame(final_dic)\n",
    "\n",
    "# Salvar o DataFrame em uma planilha CSV\n",
    "df.to_csv('planilha.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Check if n was passed as a parameter, otherwise it will default to 10\n",
    "n = 10\n",
    "if len(sys.argv) > 1:\n",
    "    n = int(sys.argv[1])\n",
    "else:\n",
    "    n = 10\n",
    "\n",
    "\n",
    "def menor_tamanho_chave(dicionario):\n",
    "    menor_chave = None\n",
    "    menor_tamanho = float('inf')  # Inicialmente, consideramos o tamanho infinito\n",
    "\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) < menor_tamanho:\n",
    "            menor_chave = chave\n",
    "            menor_tamanho = len(valor)\n",
    "\n",
    "    return menor_tamanho\n",
    "def chaves_com_tamanho_n(dicionario, tamanho):\n",
    "    chaves = []\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) > tamanho:\n",
    "            chaves.append(chave)\n",
    "    return chaves\n",
    "\n",
    "def adjusting_dict(dados_dict):\n",
    "    # Chave que você deseja dividir\n",
    "    dicionario = dados_dict\n",
    "\n",
    "    len_dic = menor_tamanho_chave(dados_dict)\n",
    "\n",
    "    chaves = chaves_com_tamanho_n(dados_dict, len_dic)\n",
    "\n",
    "    for chave in chaves:\n",
    "        # Verifica se a chave existe no dicionário\n",
    "        if chave in dicionario:\n",
    "            # Obtém a lista de valores correspondente à chave\n",
    "            lista_valores = dicionario[chave]\n",
    "\n",
    "            # Verifica se a lista tem mais de len_dic elementos\n",
    "            if len(lista_valores) > len_dic:\n",
    "\n",
    "                # Divide a lista em duas partes\n",
    "                primeira_parte = lista_valores[::2]\n",
    "                segunda_parte = lista_valores[1::2]\n",
    "\n",
    "                # Cria duas novas chaves no dicionário com as partes divididas\n",
    "                dicionario[chave + ' left-right'] = primeira_parte\n",
    "                dicionario[chave + ' right-left'] = segunda_parte\n",
    "\n",
    "                # Exclui a chave antiga\n",
    "                del dicionario[chave]\n",
    "            else:\n",
    "                dicionario[chave] = lista_valores\n",
    "\n",
    "\n",
    "    # Exibe o dicionário atualizado\n",
    "    return dicionario\n",
    "\n",
    "\n",
    "def normalize_dict(dic):\n",
    "    # Encontre o tamanho máximo entre todas as listas\n",
    "    tamanho_maximo = max(len(valor) for valor in dic.values())\n",
    "\n",
    "    # Percorra as chaves do dicionário\n",
    "    for chave, valor in dic.items():\n",
    "        if isinstance(valor, list):\n",
    "            # Se a lista for menor que o tamanho máximo, preencha com zeros\n",
    "            while len(valor) < tamanho_maximo:\n",
    "                valor.append(0)\n",
    "\n",
    "    return dic\n",
    "\n",
    "# generating sheets with result time by id execution\n",
    "def generatingSheetResultTime(id_exec):\n",
    "    df = pd.DataFrame()\n",
    "    df.to_csv('resultTime-'+id_exec+'.csv', header=True, sep=';', mode='a', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    dados_dict = {}\n",
    "\n",
    "    with open(\"./output/results/execution-\"+id_exec+\"/time.txt\") as infile:\n",
    "        for actual_line in infile:\n",
    "            partes = actual_line.strip().split(';')\n",
    "            if len(partes) == 2:\n",
    "                coluna, valor = partes[0].strip(), partes[1].strip().replace(\"s\",\"\")\n",
    "                if coluna in dados_dict:\n",
    "                    dados_dict[coluna].append(valor)\n",
    "                else:\n",
    "                    dados_dict[coluna] = [valor]\n",
    "\n",
    "    aux_dic = adjusting_dict(dados_dict)\n",
    "\n",
    "    final_dic = normalize_dict(aux_dic)\n",
    "    \n",
    "    #save the last group\n",
    "    df = pd.DataFrame(final_dic)\n",
    "    df.to_csv('resultTime-'+id_exec+'.csv', index=False)\n",
    "\n",
    "# generating a CSV file of times for the Nth execution\n",
    "for i in range(n):\n",
    "    generatingSheetResultTime(str(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# generate a csv file with the messages from the conflicts found by scenario and analysis\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Check if n was passed as a parameter, otherwise it will default to 10\n",
    "n = 5\n",
    "if len(sys.argv) > 1:\n",
    "    n = int(sys.argv[1])\n",
    "else:\n",
    "    n = 10\n",
    "\n",
    "\n",
    "def menor_tamanho_chave(dicionario):\n",
    "    menor_chave = None\n",
    "    menor_tamanho = float('inf')  # Inicialmente, consideramos o tamanho infinito\n",
    "\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) < menor_tamanho:\n",
    "            menor_chave = chave\n",
    "            menor_tamanho = len(valor)\n",
    "\n",
    "    return menor_tamanho\n",
    "def chaves_com_tamanho_n(dicionario, tamanho):\n",
    "    chaves = []\n",
    "    for chave, valor in dicionario.items():\n",
    "        if isinstance(valor, list) and len(valor) > tamanho:\n",
    "            chaves.append(chave)\n",
    "    return chaves\n",
    "\n",
    "def adjusting_dict(dados_dict):\n",
    "    # Chave que você deseja dividir\n",
    "    dicionario = dados_dict\n",
    "\n",
    "    len_dic = menor_tamanho_chave(dados_dict)\n",
    "\n",
    "    chaves = chaves_com_tamanho_n(dados_dict, len_dic)\n",
    "\n",
    "    for chave in chaves:\n",
    "        # Verifica se a chave existe no dicionário\n",
    "        if chave in dicionario:\n",
    "            # Obtém a lista de valores correspondente à chave\n",
    "            lista_valores = dicionario[chave]\n",
    "\n",
    "            # Verifica se a lista tem mais de len_dic elementos\n",
    "            if len(lista_valores) > len_dic:\n",
    "\n",
    "                # Divide a lista em duas partes\n",
    "                primeira_parte = lista_valores[::2]\n",
    "                segunda_parte = lista_valores[1::2]\n",
    "\n",
    "                # Cria duas novas chaves no dicionário com as partes divididas\n",
    "                dicionario[chave + ' left-right'] = primeira_parte\n",
    "                dicionario[chave + ' right-left'] = segunda_parte\n",
    "\n",
    "                # Exclui a chave antiga\n",
    "                del dicionario[chave]\n",
    "            else:\n",
    "                dicionario[chave] = lista_valores\n",
    "\n",
    "\n",
    "    # Exibe o dicionário atualizado\n",
    "    return dicionario\n",
    "\n",
    "\n",
    "def normalize_dict(dic):\n",
    "    # Encontre o tamanho máximo entre todas as listas\n",
    "    tamanho_maximo = max(len(valor) for valor in dic.values())\n",
    "\n",
    "    # Percorra as chaves do dicionário\n",
    "    for chave, valor in dic.items():\n",
    "        if isinstance(valor, list):\n",
    "            # Se a lista for menor que o tamanho máximo, preencha com zeros\n",
    "            while len(valor) < tamanho_maximo:\n",
    "                valor.append(0)\n",
    "\n",
    "    return dic\n",
    "\n",
    "# generating sheets with result time by id execution\n",
    "def generatingSheetResultTime(id_exec):\n",
    "    df = pd.DataFrame()\n",
    "    df.to_csv('conflicts_log-'+id_exec+'.csv', header=True, sep=';', mode='a', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    dados_dict = {}\n",
    "    \n",
    "    with open(\"./output/results/execution-\"+id_exec+\"/conflicts_log.txt\") as infile:\n",
    "        aux_list = [\"0\" for i in range(6)]\n",
    "    \n",
    "        for actual_line in infile:\n",
    "            \n",
    "            partes = actual_line.strip().split(': [')\n",
    "           \n",
    "            if len(partes) == 2:\n",
    "                coluna, valor = partes[0].strip(), partes[1].strip()\n",
    "                \n",
    "                if coluna in dados_dict:\n",
    "                    dados_dict[coluna].append(valor)\n",
    "                else:\n",
    "                    dados_dict[coluna] = [valor]\n",
    "\n",
    "    \n",
    "    aux_dic = adjusting_dict(dados_dict)\n",
    "\n",
    "    final_dic = normalize_dict(aux_dic)\n",
    "    \n",
    "    #save the last group\n",
    "    df = pd.DataFrame(final_dic)\n",
    "    df.to_csv('conflicts_log-'+id_exec+'.csv', index=False)\n",
    "\n",
    "# generating a CSV file of conflicts for the Nth execution\n",
    "for i in range(n):\n",
    "    generatingSheetResultTime(str(i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
